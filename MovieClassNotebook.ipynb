{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "\n",
    "class MovieDataset(BaseModel):\n",
    "    \"\"\"Class to handle downloading, extracting, and loading the CMU Movie Dataset.\n",
    "\n",
    "    Attributes:\n",
    "        base_url (str): URL for downloading the dataset.\n",
    "        dataset_filename (str): Name of the dataset archive file.\n",
    "        download_dir (Path): Directory where dataset will be downloaded.\n",
    "        extracted_dir (Path): Directory where dataset will be extracted.\n",
    "        dataset_path (Path): Full path to the downloaded archive.\n",
    "        dataframes (Dict[str, Optional[pd.DataFrame]]): Dictionary holding all loaded DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    base_url: str = \"http://www.cs.cmu.edu/~ark/personas/data/\"\n",
    "    dataset_filename: str = \"MovieSummaries.tar.gz\"\n",
    "    download_dir: Path = Path(\"downloads\")\n",
    "    extracted_dir: Path = download_dir / \"MovieSummaries\"\n",
    "    dataset_path: Path = download_dir / dataset_filename\n",
    "\n",
    "    # Dictionary to store all dynamically loaded datasets\n",
    "    dataframes: Dict[str, Optional[pd.DataFrame]] = {}\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)  # Allow Pandas DataFrames\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MovieDataset class.\n",
    "        - Creates necessary directories.\n",
    "        - Downloads dataset if missing.\n",
    "        - Extracts dataset if needed.\n",
    "        - Dynamically loads all `.tsv` and `.txt` files into Pandas DataFrames.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure the download directory exists\n",
    "        self.download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Download dataset if it does not exist\n",
    "        if not self.dataset_path.exists():\n",
    "            self.download_dataset()\n",
    "\n",
    "        # Extract dataset if it has not been extracted\n",
    "        if not self.extracted_dir.exists():\n",
    "            self.extract_dataset()\n",
    "\n",
    "        # Load all available dataset files dynamically\n",
    "        self.load_all_datasets()\n",
    "\n",
    "    def download_dataset(self):\n",
    "        \"\"\"Downloads the dataset from the specified URL if it does not already exist.\"\"\"\n",
    "        print(f\"Downloading {self.dataset_filename}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(self.base_url + self.dataset_filename, stream=True)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with open(self.dataset_path, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "\n",
    "            print(\"Download complete.\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "\n",
    "    def extract_dataset(self):\n",
    "        \"\"\"Extracts the dataset archive into the designated directory.\"\"\"\n",
    "        print(\"Extracting dataset...\")\n",
    "\n",
    "        try:\n",
    "            with tarfile.open(self.dataset_path, \"r:gz\") as tar:\n",
    "                tar.extractall(path=self.download_dir)\n",
    "            print(\"Extraction complete.\")\n",
    "        except tarfile.TarError as e:\n",
    "            print(f\"Error extracting dataset: {e}\")\n",
    "\n",
    "    def load_all_datasets(self):\n",
    "        \"\"\"\n",
    "        Dynamically loads all `.tsv` and `.txt` files from the extracted directory into Pandas DataFrames.\n",
    "        - Each dataset is stored in a dictionary (`dataframes`) using the filename (without extension) as the key.\n",
    "        \"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "\n",
    "        if not self.extracted_dir.exists():\n",
    "            print(f\"Error: Extracted directory {self.extracted_dir} does not exist.\")\n",
    "            return\n",
    "\n",
    "        for file_path in self.extracted_dir.glob(\"*\"):\n",
    "            if file_path.suffix in [\".tsv\", \".txt\"]:  # Load only relevant file types\n",
    "                self.load_dataset(file_path, sep=\"\\t\")\n",
    "        \n",
    "    def load_dataset(self, file_path: Path, sep: str = \"\\t\"):\n",
    "        \"\"\"\n",
    "        Loads a dataset file into a Pandas DataFrame and stores it in `dataframes`.\n",
    "\n",
    "        Args:\n",
    "            file_path (Path): Path to the dataset file.\n",
    "            sep (str): Separator used in the file (default is tab-separated).\n",
    "        \"\"\"\n",
    "        dataset_name = file_path.stem  # Extract filename without extension\n",
    "\n",
    "        print(f\"Checking file: {file_path}\")  # Debugging line\n",
    "\n",
    "        if file_path.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=sep, header=None)\n",
    "                object.__setattr__(self, dataset_name, df)  # Dynamically set as attribute\n",
    "                self.dataframes[dataset_name] = df  # Store in dictionary\n",
    "                print(f\"Loaded dataset: {dataset_name}, Shape: {df.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR while loading {file_path}: {e}\")\n",
    "                self.dataframes[dataset_name] = None\n",
    "        else:\n",
    "            print(f\"ERROR: {file_path} NOT FOUND!\")\n",
    "            self.dataframes[dataset_name] = None\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    movie_data = MovieDataset()\n",
    "#\n",
    "#    # Accessing instance attributes\n",
    "#    print(movie_data.movie_metadata.head())  # Metadata\n",
    "#    print(movie_data.plot_summaries.head())  # Plot summaries\n",
    "#    print(movie_data.character_metadata.head())  # Character metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g8/9bxfvb6s2sn1rcndgh56ns3w0000gn/T/ipykernel_85493/1269244418.py:81: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=self.download_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n",
      "Loading datasets...\n",
      "Checking file: downloads/MovieSummaries/plot_summaries.txt\n",
      "Loaded dataset: plot_summaries, Shape: (42303, 2)\n",
      "Checking file: downloads/MovieSummaries/movie.metadata.tsv\n",
      "Loaded dataset: movie.metadata, Shape: (81741, 9)\n",
      "Checking file: downloads/MovieSummaries/name.clusters.txt\n",
      "Loaded dataset: name.clusters, Shape: (2666, 2)\n",
      "Checking file: downloads/MovieSummaries/README.txt\n",
      "Loaded dataset: README, Shape: (52, 1)\n",
      "Checking file: downloads/MovieSummaries/character.metadata.tsv\n",
      "Loaded dataset: character.metadata, Shape: (450669, 13)\n",
      "Checking file: downloads/MovieSummaries/tvtropes.clusters.txt\n",
      "Loaded dataset: tvtropes.clusters, Shape: (501, 2)\n"
     ]
    }
   ],
   "source": [
    "test_data = MovieDataset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
